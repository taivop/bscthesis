\documentclass[a4paper,12pt]{report}


%
% Bunch of useful packages
%
\usepackage{etex}
\usepackage[utf8]{inputenc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{pst-plot}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{floatflt}
\usepackage{wrapfig}
\usepackage{endnotes}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{endnotes}
\usepackage{pdflscape}
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{array}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{enumitem}

\usepackage[font={small, sl}]{caption}

%
% Listings
%
\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
\lstset{
	basicstyle=\scriptsize\ttfamily,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
	tabsize=2,
	extendedchars=true,
	breaklines=true,
	keywordstyle=\color{red},
	frame=b,         
	stringstyle=\color{white}\ttfamily,
	showspaces=false,
	showtabs=false,
	xleftmargin=17pt,
	framexleftmargin=17pt,
	framexrightmargin=5pt,
	framexbottommargin=4pt,
	showstringspaces=false
}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}

%
% Chapter titles
%
\usepackage{titlesec}
\titleformat{\chapter}[hang] 
{\normalfont\huge\bfseries}{\thechapter}{1em}{} 

%
% Theorems and Definitions
%
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]

%
% Margins of all kinds
%
\hypersetup{pdfborder={0 0 0 0}}
\setlength\parindent{0mm}
\setlength\parskip{3mm}

\newenvironment{pitemize}{
\vspace{-5mm}
\begin{itemize}
 	\setlength{\itemsep}{1pt}
	\setlength{\parskip}{0pt}
 	\setlength{\parsep}{0pt}
}{
	\end{itemize}
	\vspace{-8mm}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Width of A4 is 8.27in (210mm) We have an inside margin of 1.5 in and outside margin of 1in
%% This leaves 5.77in for text width
\oddsidemargin=0.5in
\evensidemargin=0.0in
\textwidth=5.77in
\headheight=0.0in
\topmargin=0.0in
\textheight=9.0in
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newenvironment{sitemize}
{\vspace{-2mm}\begin{list}{\textbullet}{%
    \setlength{\itemsep}{0pt}%
    \setlength{\parsep}{0pt}%
    \setlength{\topsep}{0pt}%
    \setlength{\parskip}{0pt} %
    \setlength{\labelwidth}{0pt}%
    \setlength{\labelsep}{0.05in} %
    \setlength{\leftmargin}{5pt} %
    \renewcommand{\labelitemi}{\textbullet}}}
  {\vspace{-2mm}\end{list}}

\newenvironment{mitemize}
{\begin{list}{\textbullet}{%
    \setlength{\itemsep}{0pt}%
    \setlength{\parsep}{0pt}%
    \setlength{\topsep}{2mm}%
    \setlength{\parskip}{0pt} %
    \setlength{\labelwidth}{0pt}%
    \setlength{\labelsep}{0.05in} %
    \setlength{\leftmargin}{8.5mm} %
    \renewcommand{\labelitemi}{\textbullet}}}
  {\end{list}}


\newcommand\epigraph[3]{
\vspace{1em}\hfill{}\begin{minipage}{#1}{\begin{spacing}{0.9}
\small\noindent\textit{#2}\end{spacing}
\vspace{1em}
\hfill{}{#3}}\vspace{2em}
\end{minipage}}


%
% Title
%
\newcommand{\thesistitle}{Implementing and studying the behaviour of a biologically realistic neuron model}
\newcommand{\thesistitleEST}{Bioloogiliselt realistliku neuroni mudeli ehitamine ja selle käitumise uurimine}



\begin{document}
\begin{center}
	{\Large
	University of Tartu\\
	Faculty of Mathematics and Computer Science\\
	Institute of Computer Science\\
	Computer Science\\}
	
	\vspace{2.5cm}
	
	{\LARGE Taivo Pungas}\\
	\vspace{0.5cm}
	\begin{spacing}{2}{\Huge\bf \ \ \ \thesistitle}\end{spacing}
	\vspace{0.5cm}
	{\LARGE Bachelor's thesis (9 ECTS)}
\end{center}
\vspace{3cm}
\hspace{7.2cm}{\Large Supervisors: Dr. Raul Vicente\\}
\vspace{-0.5cm}

\hspace{10.4cm}{\Large Dr. Jaan Aru \\}

\ \\
Author: .................................................. "......." May 2015\\
Supervisor: ............................................. "......." May 2015\\
Supervisor: ............................................. "......." May 2015\\
\ \\
Approved for defense\\
Professor: ............................................... "......." May 2015\\
\ \\
\begin{center}
	{\Large Tartu 2015}
\end{center}
\thispagestyle{empty}
\pagebreak


%%%
% Abstract
%%%


{\textbf
{\Large \thesistitle}}

\textbf{Abstract:}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam a ultricies sem, vel gravida dolor. Vestibulum lacinia nulla nec massa mollis eleifend. Etiam scelerisque erat in ligula dapibus, et eleifend enim vulputate. Integer quis lobortis neque, eget efficitur felis. Vestibulum vel consequat quam. Praesent volutpat eget massa rutrum fringilla. Maecenas fringilla turpis ut dignissim luctus. Nunc lectus metus, dapibus eu enim ut, rhoncus blandit ligula. Praesent gravida ullamcorper est id pharetra. Mauris tempus ornare sollicitudin. Sed feugiat nec turpis mattis sollicitudin. Aliquam non orci sit amet lorem rutrum euismod et non orci. Aliquam egestas aliquam blandit. Cras quis convallis neque. Integer fringilla pharetra condimentum. Fusce erat eros, pulvinar sit amet orci in, consectetur accumsan eros.

\textbf{Keywords:} single neuron model, numerical simulation, synaptic plasticity, unsupervised learning, probabilistic neurotransmitter release

\vspace{1.5cm}



{\textbf
{\Large \thesistitleEST}}

\textbf{Lühikokkuvõte:}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam a ultricies sem, vel gravida dolor. Vestibulum lacinia nulla nec massa mollis eleifend. Etiam scelerisque erat in ligula dapibus, et eleifend enim vulputate. Integer quis lobortis neque, eget efficitur felis. Vestibulum vel consequat quam. Praesent volutpat eget massa rutrum fringilla. Maecenas fringilla turpis ut dignissim luctus. Nunc lectus metus, dapibus eu enim ut, rhoncus blandit ligula. Praesent gravida ullamcorper est id pharetra. Mauris tempus ornare sollicitudin. Sed feugiat nec turpis mattis sollicitudin. Aliquam non orci sit amet lorem rutrum euismod et non orci. Aliquam egestas aliquam blandit. Cras quis convallis neque. Integer fringilla pharetra condimentum. Fusce erat eros, pulvinar sit amet orci in, consectetur accumsan eros.

\textbf{Võtmesõnad:} taivo, raul, jaan, mudel, neuro, värk


\
\thispagestyle{empty}
\pagebreak
% % % %
% Chapter: Acknowledgements
% % % %

\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}

I would like to thank the comfy chairs I was able to sit in while writing my thesis. %TODO

This work was supported by the IT Academy program.



\tableofcontents
\newpage



% % % %
% Chapter: Introduction
% % % %

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}


Your brain is a complex organ capable of very sophisticated thought. Even though the chess-playing supercomputer Deep Blue won the reigning world champion Garry Kasparov, Kasparov was also able to do anything human beings do every day, whereas the computer could only play chess. The human brain is remarkable not for its ability to do one particular thing very well, be it playing chess, reading an article in Nature, preparing a six-course meal, or slam dunking in a basketball game. It is remarkable for being able to do all of those things, and much more.

However, gaining understanding of the brain's information processing mechanisms remains one of the major scientific challenges today. A key aspect of the brain is neural plasticity, its ability to change itself over time as new knowledge and experience are accumulated and processed. Thus, neuronal plasticity, and specifically synaptic plasticity (change in the strength of neuronal connections) is thought to underlie our ability for learning and memory. To understand the dynamics of these neuronal changes, computational models are built to assess how well they explain the brain's behaviour as observed in experiments in neuroscience and, more generally, how the models perform in a variety of tasks.

Artificial systems with significant capacity for learning have recently been created using deep convolutional neural networks, performing well in tasks ranging from playing video games \cite{mnih2015human} to describing scenes in natural language \cite{karpathy2014deep}. The neuron models used in these systems are very simple, often composing of a scalar product of the inputs and weights, and a thresholding function. However, evidence from brains has shown that each neuron is a much more complex and powerful information processing unit. There is a clear gap between the learning rules used in networks of neurons, and biophysical mechanisms of learning at the cellular level. The primary goal of this thesis is to help narrow this gap by studying the behaviour of a biologically realistic neuron model. To do so, a detailed implementation of such a model was built and some of its learning capabilities tested. 

In particular, the approach taken in this thesis is to show by numerical simulations neuron behaviour resulting from basic principles of neuronal biophysics. This work builds on top of a plasticity model (described as a set of differential equations) previously published in \cite{yeung2004synaptic} and which has been shown to produce a range of neural plasticity phenomena. 

The model allows inspection of three aspects which are of particular interest. Firstly, the changing of neural plasticity--metaplasticity--is important for producing well-known learning phenomena. Secondly, probabilistic neurotransmitter release which causes signal transmission failure might have an effect of increasing information transfer efficiency. Thirdly, the neuron's ability to perform principal component analysis (PCA), a well-known unsupervised learning algorithm, is tested. This is motivated by prior work showing that very simple neuron models are capable of PCA \cite{oja2008oja}.

After a brief overview of the background and related work in Chapter 1 and describing the methods used in Chapter 2, Chapter 3 starts with validation of the behaviour of the implemented model against previously published work. Then, the neuron input-output relation is characterised for various kinds of input. As an addition to the model, the effect of probabilistic neurotransmitter release is studied. Finally, a simple task is simulated to study the neuron's ability to implement PCA.


% % % %
% Chapter: Background
% % % %

\chapter{Background and related work}


\section{Overview of related neuroscience}

\subsection{Neurobiology of the brain} % Biology of the human brain
The brain, a part of the nervous system, is the central information processing organ in animals. The computational properties of the brain arise from the heavily interconnected networks and sub-networks of specialised cells called \emph{neurons}. It has been approximated in \cite{herculano2009human} that the human brain consists of $10^{11}$ neurons with $10^{15}$ inter-neuronal connections called \emph{synapses}. This work focuses on modelling one of many different types of neurons - pyramidal neurons.


\subsection{Neurons as computational units}
Every model of a brain that aspires for biophysical reality must include a way to model the behaviour of neurons and synapses. Each neuron can be seen as a small unit performing computation on some inputs and producing corresponding output. There are many computational models of neurons, with the level of detail ranging from complex multi-compartment models to very simple models such as those used in many artificial neural networks (ANNs). 

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/fig1.pdf}
    \caption{Schematic of a pyramidal neuron.}
    \label{fig:pyramidal}
\end{figure}

Information flow through a neuron follows a path from the inputs to the output, with integration of information in between, as shown in Figure~\ref{fig:pyramidal}. A rough mapping of the corresponding neuron anatomy is also shown.


\subsubsection{Input}

The region of connection between two neurons is called a \emph{synapse}, with information flowing from the axon of the \emph{presynaptic} neuron to a dendrite of the \emph{postsynaptic} neuron. When the presynaptic neuron produces significant output, neurotransmitters are released from the presynaptic neuron into the \emph{synaptic cleft}, a small gap between the presynaptic and postsynaptic neurons at the synapse.
Neurotransmitters then bind to receptors on the dendrite of the postsynaptic neuron causing ion channels to open. This causes ion flow across the membrane which causes a voltage change at the dendrite of the postsynaptic neuron. However, the neurotransmitter release is unreliable: upon some presynaptic spikes no neurotransmitter is relased. This effect is called \emph{proabilistic neurotransmitter release} (PR) or \emph{synaptic failure}.

Synapses can be divided into two classes: input arriving at \emph{excitatory} synapses tends to increase the output of the postsynaptic neuron, input to \emph{inhibitory} synapses tends to make it fire less.

\subsubsection{Computation}
A single neuron can be viewed as an integrator of information over time, over different inputs, or both. At each synapse, a neuron has some intrinsic biological components that determine how much this synapse should affect the output of the postsynaptic neuron. \emph{Synaptic weights} are key parameters used in modelling such biological components, and the modification of synaptic weights is the basis of learning in neurons and thus, in the brain.

After the contribution of input from each synapse is computed, this information must be integrated to produce the output of the neuron. An overview of some of the most common neural models of information integration will be given in Section~\ref{modelsofintegration}.

\subsubsection{Output}

Output of a neuron takes the form of \emph{action potentials} or \emph{spikes}, rapid increases in membrane voltage followed by a return to the equilibrium voltage. Information about action potentials is conducted from a neuron to the dendrites of other neurons using the axon. Axons are usually not part of single neuron models as the output of the modelled neuron is studied in and of itself.

\section{Prior work}


There are many computational models of neurons, with the level of detail ranging from compartmental to very simple models of a neuron such as those used in ANNs. Using these models, it is possible to simulate neurons in a computer and investigate the models' behaviour in a very controlled and detailed way. As opposed to real experiments that are hindered by many difficulties, using a simulation approach allows detailed investigation and control of input and all parameters.

Two aspects of neurons can be modelled and used in different combinations: the mechanism for translating input signals into output (information integration) and the mechanism for updating synaptic weights (synaptic plasticity).


\subsection{Models of neural information integration}
\label{modelsofintegration}

Some of the best known models, from least to most detailed, are the McCulloch-Pitts \cite{mcculloch1943logical}, integrate-and-fire (IF), and Hodgkin-Huxley \cite{hodgkin1952quantitative} models. In the MP model which is widely utilised in ANNs, a weighted sum of inputs is calculated, to which a sigmoid function is applied to produce the output of the neuron. In the IF and HH models the neuron is modelled as an electric circuit, and then corresponding differential equations are used to calculate the output of the neuron.

\subsection{Models of synaptic plasticity}

Synaptic plasticity underpins learning in a neuron. It is possible to implement learning directly by calculating the error in the neuron's output: the difference between expected output (the correct label) and the neuron's prediction. Examples of these simple phenomenological rules include Hebbian rules (summarised as "Cells that fire together, wire together") \cite{hebb1952organisation}, Oja's rule (shown to produce PCA computation regardless of its simplicity) \cite{oja2008oja} and others, and are typically used in conjunction with MP neuron models. Learning rules can also be derived to minimise information-theoretic measures such as Fisher information \cite{echeveste2014generating}.

Alternatively, learning can also arise from biophysical properties of more detailed biological models of synapses. % TODO expand

\subsection{Probabilistic neurotransmitter release}

Recordings from hippocampal pyramidal neurons have shown synaptic transmission to be unreliable due to failure of release of neurotransmitter into the synaptic cleft upon a single presynaptic spike \cite{stevens1995facilitation}. Furthermore, the probability of successful release $p$ has been shown to have a wide distribution with predominant low values ($p < 0.4$) in the hippocampus \cite{murthy1997heterogeneous}.

PR with $p<1$ has been shown to have a beneficial effect of increasing the information-carrying efficiency of a single synapse, caused primarily by a decrease in noise \cite{goldman2004enhancement}. However, \cite{guo2012population} conclude that for a population of neurons, encoding performance is increased in the presence of weak noise and decreased in the presence of strong noise, adding that "several important synaptic parameters . . . have significant effects on the performance of the population rate coding".

\subsection{Learning in single neuron models}

\subsubsection{Calcium-based unified plasticity models} % TODO maybe rename this subsection
An overview of calcium-based biophysical models of synaptic plasticity is given in \cite{shouval2007models}. The model used in this work is notable for exhibiting several synaptic plasticity phenomena within one model, providing a unified model of synaptic plasticity \cite{shouval2002unified}. Other unifying models have been proposed with different underlying mechanisms and differing results.

Building on top of the \cite{shouval2002unified} model, it was shown in \cite{yeung2004synaptic} that adding a stabilisation mechanism (metaplasticity) synapses become stable, avoiding positive feedback loops that would result in very high synaptic weights. In addition, synaptic competition is achieved, allowing synaptic weights to "reflect the statistical properties of the inputs" \cite{yeung2004synaptic}. This allows the neuron to learn complex input patterns while maintaining a relatively constant range of output.

The model used in this thesis will be discussed in more detail in Section~\ref{methods-modelused}.

\subsubsection{Unsupervised learning}

Several single neuron models have been shown to be capable of unsupervised learning: simple artificial neurons can do PCA when trained according to the Oja rule \cite{oja2008oja}, the tempotron model \cite{gutig2006tempotron} is able to learn spike timing-based patterns, the SKAN model \cite{afshar2014racing} is capable of unsupervised classification of patterns. However, none of the aforementioned models are based on biophysical principles, and no other biophysical models have been shown to do unsupervised learning on the single neuron level.



Having taken a look at the state of the art, the main goals of the work can be formulated as:
(a) implementing the chosen neuron model and validating its behaviour,
(b) exploring the learning behaviour of this model,
(c) finding effects of probabilistic neurotransmitter release on behaviour, and
(d) testing whether the neuron can perform PCA.




% % % %
% Chapter: Methods
% % % %
\chapter{Methods}

\section{Neuron model used in simulations}
\label{methods-modelused}

The neuron model used in this work follows the model of a single neuron published in \cite{yeung2004synaptic}. An integrate-and-fire model of information integration is used with a calcium-dependent model of synaptic plasticity \cite{shouval2002unified}. No dendritic distance is simulated, i.e. the possibility that some synapses are situated further from the soma is not taken into account. In this section, an overview of the most important aspects of the model will be given. The total number of synapses is 120, with 100 excitatory synapses and 20 inhibitory synapses.



\subsection{Input generation}
Rather than simulating presynaptic neurons, inputs to the model are simulated as $N$ spike trains, one for each excitatory synapse, with a given mean firing rate $r$ specified independently for each synapse. Due to the way information is represented in computers, time is discretised into time-steps of length $dt$ regardless of the continuous nature of the biophysical processes. Each spike train one binary number per time-step: 1 if a spike occurred and 0 otherwise. The spike trains are produced in a homogeneous Poissonian process, i.e. the occurrence of a spike is independent of the time since the last spike, and the average rate of spikes remains constant over time.
Correlation between inputs is achieved by specifying the correlation parameter $0 \leq c \leq 1$, and using code from \cite{macke2009} to generate correlated homogeneous Poissonian spike trains. For $c=1$, all synapses receive identical input; for $c=0$, all synapses receive independently generated spike trains.

In all simulations, inputs to inhibitory synapses are generated as uncorrelated Poissonian spike trains with a mean firing rate of 10Hz.


\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/methods_sample_input.eps}
    \caption{Sample input to 30 synapses, generated for a 1000ms period with a 0.1ms time-step. In this figure, black bars indicate timesteps at which spiketrains (running horizontally) contain spikes. Synapses 1-10 receive 5Hz uncorrelated input, synapses 11-20 receive 40Hz correlated input with $c=0.8$, synapses 21-30 receive 40Hz uncorrelated input.}
    \label{fig:methods_sample_input}
\end{figure}




\subsection{Integrate-and-fire model}

The postsynaptic neuron is modelled as a Traub-Miles cell, which is a Hodgkin-Huxley-like model for estimating postsynaptic voltage. The equations and parameters from \cite{ermentrout1998fine} are used, with one significant difference in parameters: the basal current $I_0$, which causes spontaneous voltage increase, is set to $0$. Compared to Hodgkin-Huxley, the Traub-Miles model includes a more detailed simulation of Na\textsuperscript{+} and K\textsuperscript{+} currents by modelling the dependence of the gating of abovementioned channels on postsynaptic voltage.

The core of the integrate-and-fire model used in this thesis are five currents affecting postsynaptic voltage: a voltage-increasing current caused by presynaptic spikes at excitatory synapses, a voltage-decreasing current caused by presynaptic spikes at inhibitory synapses, Na\textsuperscript{+} and K\textsuperscript{+} currents not directly affected by presynaptic spiking, and a leak current, which tends to return voltage to the resting potential $V_{rest}=-65\mathrm{mV}$. When postsynaptic voltage $V_{post}$ reaches the threshold $V_{thres}=-55\mathrm{mV}$, a postsynaptic spike is recorded, and $V_{post}$ is set to the spiking voltage $V_{spike}=40\mathrm{mV}$. In the next time-step, $V_{post}$ is returned to the reset potential $V_{reset}=-65\mathrm{mV}$, from where it returns to the baseline voltage.

As an addition to the Traub-Miles model, spike-frequency adaptation is implemented following \cite{yeung2004synaptic}: the resting voltage is decreased by 2mV upon each postsynaptic spike, and decays back to the baseline with a time constant of $100\mathrm{ms}$. This temporarily increases the gap between resting and threshold voltages, requiring more presynaptic input to reach the threshold and thus decreasing firing rate.




\subsection{Model of synapses}

Excitatory and inhibitory synapses are modelled by ion gates controlled by the respective neurotransmitters: glutamate and gamma-aminobutyric acid (GABA). Upon each spike at an excitatory synapse, the opening of glutamate-controlled ion channels at dendrites is simulated, with the degree of opening simulated as a sum of two decaying exponentials peaking at the time of spike. The resulting ion flow tends to drive $V_{post}$ towards the excitatory reversal potential $V_{rev,exc}=0\mathrm{mV}$. Similarly, upon each spike at an inhibitory synapse, the opening of GABA-controlled ion channels is simulated, and the resulting ion flow tends to drive $V_{post}$ towards the inhibitory reversal potential $V_{rev,inh}=-65\mathrm{mV}$. Spikes at excitatory synapses have a larger relative influence on $V_{post}$) due to the higher simulated conductivity (maximum throughput of ions) of glutamate-controlled ion channels.

The effect of presynaptic spikes on the gating of glutamate-controlled and GABA-controlled channels is simulated according to the model from \cite{borgers2008gamma}.

%FIGURE: spatial and temporal summation of EPSP-s, causing a spike (action potential) vs not causing a spike. Also on same timeline, an IPSP. Below, show excitatory and inhibitory spikes as raster plots. I think Julius had a good figure for this.




\subsection{Plasticity}

In addition to components mentioned in the previous section, the extent to which presynaptic spikes influence postsynaptic voltage is modelled by the synaptic weights $\boldsymbol{W}=(W_i)$, with $i$ denoting the number of the synapse. The modification of these weights is the basis of learning in this model: increasing $W_i$ will increase the effect spikes at the $i$-th synapse have on postsynaptic voltage, and vice versa. In this model, only excitatory synapses are plastic, i.e. the weights of inhibitory synapses remain unchanged.

The mechanism for modifying synaptic weights follows the Ca\textsuperscript{2+}-dependent model from \cite{shouval2002unified}, where the aim was "to construct a model based on a minimal number of assumptions". The model detects coincidences of presynaptic and postsynaptic spikes: the weight of synapse $i$ will be increased if the presynaptic spike occurs right before the postsynaptic spike (i.e. there is reason to believe the postsynaptic spike was \emph{caused} by the input), and decreased otherwise. The process of spike-timing-dependent plasticity (STDP) captures this idea that learning should depend on the exact timing on postsynaptic and presynaptic spikes. It was shown in \cite{shouval2002unified} that the plasticity model used in this work implements STDP in a way similar to what has been observed in experiments in neuroscience.

The mechanism for detecting coincidences relies on N-methyl-D-aspartic acid (NMDA) receptors. NMDA receptors control gates that can allow calcium ions to flow into the neuron, which in turn causes an increase of the synaptic weight. The extent to which the calcium channels open depend on two factors: concentration of glutamate (the neurotransmitter that also causes voltage increases upon spikes at excitatory synapses) and dendritic voltage caused by a back-propagating action potential (BPAP). The former relays information about presynaptic spikes: if a spike has occurred recently, the level of glutamate at the synapse is at a high level. The latter signifies a postsynaptic spike: when the postsynaptic neuron fires, an increase in voltage is propagated to the dendrites. The influx of calcium is dependent on the exact timing of spikes because both glutamate concentration and BPAP voltage decay in time. The synaptic weights' update depends on the level of intracellular calcium as shown in Equation~\ref{eq:weightupdate}, with shapes of $\Omega$ and $\eta$ shown in Figure~\ref{fig:methods_eta_omega}.

\begin{equation}
\frac{dW_i}{dt} = \eta ([Ca]_i) (\Omega([Ca]_i) - \lambda W_i)
\label{eq:weightupdate}
\end{equation}

This mechanism allows the synaptic weights to increase when there has been a suitably timed pair of presynaptic and postsynaptic spikes. However, \cite{shouval2002unified} show that the same mechanism also accounts for decreasing and stable synaptic weights through the effect of $\Omega$: at medium levels of Ca\textsuperscript{2+}, weights are decreased ($\Omega<0.5$), and at low levels of Ca\textsuperscript{2+}, synaptic weights remain unchanged ($\Omega$ is constant at 0.5). The learning rate $\eta$ also depends on calcium: higher levels of Ca\textsuperscript{2+} elicit larger changes in synaptic weights. $\lambda$, together with the value of $\Omega$ in the range of low calcium, determine the stable point of synaptic weights.

The phenomenon of synaptic weights increasing due to learning is named long-term potentiation (LTP), and similar decreasing is named long-term depression (LTD). LTD and LTP are the methods the neuron uses to assign different synaptic weights based on the input to that synapse.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/methods_eta_omega.eps}
    \caption{A. The $\Omega$ function. B. The learning rate function $\eta$.}
    \label{fig:methods_eta_omega}
\end{figure}


\subsubsection{Metaplasticity}
\emph{Metaplasticity} refers to plasticity of synaptic plasticity - the dependence of plasticity on the history of the previous activity of the neuron - which may play a role in LTP, LTD, and other learning mechanisms. In the scope of this thesis it refers to the changing of NMDA receptor (NMDAR) conductance as a function of output voltage, which leads to changes in the calcium influx and therefore changes in the sign and magnitude of changes in synaptic weights. The model of metaplasticity follows that of \cite{yeung2004synaptic}, where the NMDAR conductance $g$ is the same for all synapses and is updated according to the following equation: $$ \frac{dg}{dt}=-(k_- (V-V_{rest})^2 + k_+)g + k_+ g_t $$
Here, $g_t$ shows the total supply of NMDARs available in the internal pool of the neuron. $k_+$ and $k_-$ are kinetic constants describing the speed of insertion and removal of NMDARs into synapses.

Metaplasticity can be disabled in any simulation, resulting in a constant value of $g$.





\section{Implementation details}

The model was implemented and data analysis conducted in MATLAB; source code is attached as an online supplementary to the thesis. The simulations were carried out in part in the High Performance Computing Center of University of Tartu.

For updating values according to the differential equations, the Euler method is used with a 0.1ms time-step length, chosen as the highest value allowing numerical stability. At the start of each simulation, some parameters are initialised to previously found equilibrium values to avoid unstable behaviour. Input is generated in 1-second blocks for ease of implementation, and the release of neurotransmitters caused by each presynaptic spike is assumed to last 1 ms. The computational complexity is linear with respect to the total simulation time; the computation time required for $x$ seconds of simulated neuron time varies between $2x$ and $15x$ seconds on the machines used for simulation and depends on the amount of input to the neuron.

Unless mentioned otherwise, learning speed was increased by a factor of 100 by multiplying the values of learning rate $\eta$ and NMDA receptor kinetic constants $k_+$ and $k_-$ by 100. According to \cite{yeung2004synaptic}, the fixed points of the system do not change upon speeding up the simulations by this factor.











% % % %
% Chapter: Results
% % % %
% Useful link for figures: http://tex.stackexchange.com/questions/37581/latex-figures-side-by-side
\chapter{Results}

\section{Validation of model behaviour}

As a prerequisite to analysing more complex behaviour of the model, it is necessary to make sure the model is implemented correctly. Syntax and other obvious errors aside, the simplest way to do this is to observe the behaviour of the model under some conditions and compare the result with previously published results on the same model in the same conditions. As \cite{shouval2002unified} and \cite{yeung2004synaptic} form the basis of the model, the behaviour of the model implemented in this thesis will be compared to these two sources. Attention will also be paid to keeping parameters and outputs in biophysically realistic ranges.

\subsection{Input-output relationship}
\label{subsec:iocurve}

The input-output curve of a neuron is one of the most important characteristics, showing the output of the neuron to a specified amount of input. The exact scale of the curve is highly dependent on the specific parameters used. In particular, EPSP amplitude has a major effect on this relationship. 
Figure~\ref{fig:valid_iocurve_vs_epsp_grid} shows the curve for 6 different neurons, each with a different EPSP amplitude. It is evident that the relationship is linear for a range of EPSP amplitudes, which is consistent with the findings of \cite{yeung2004synaptic} for simulations with metaplasticity enabled. For low values of EPSP amplitude and input rate, there is no output, indicating subthreshold activity: postsynaptic voltage never reaches the threshold required for producing a spike.

Increasing EPSP amplitude by a factor, the curve remains linear, but output rates are scaled up by a similar factor. This means the plausible EPSP voltage range is not much constrained by considering the shape of the input-output curve, and EPSP voltage can be tuned to scale the output of the neuron according to other considerations. The EPSP amplitude producing the closest curve to that of \cite{yeung2004synaptic} lies between 1.0mV and 2.0mV. However, another crucial consideration here is biophysical reality: for completely unstructured input (noise) at 30Hz, a firing rate of even 40Hz is excessive. For this reason, in further simulations EPSP voltage was set to 3.0mV, a value found to produce good results in other informal parameter sweeps.


\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/valid_iocurve_vs_epsp_grid.eps}
    \caption{Input-output relationship of the neuron for different values of EPSP amplitude parameter (shown above each plot). Each data point in a plot is one experiment, in which input to the neuron consists of uncorrelated spike trains with mean input rate per synapse shown on the x-axis. Due to plasticity and metaplasticity, output rate varies until a fixed point is reached; for this reason, output rate is measured over the last 500 seconds in a 10000-second experiment.}
    \label{fig:valid_iocurve_vs_epsp_grid}
\end{figure}




\subsection{Spike timing-dependent plasticity}

A spike-timing-dependent plasticity curve is important aspect of neuron models: it shows how the neuron determines the sign and magnitude of  synaptic weight updates in response to the relative timing of pre- and postsynaptic spikes. In the model used here, two parameters significantly affect STDP curve shape: BPAP amplitude and the amount of glutamate released on the presynaptic side upon each spike. The shape of STDP curve using parameters from \cite{yeung2004synaptic} is given in Figure~\ref{fig:valid_stdp}A. For comparison, Figure~\ref{fig:valid_stdp}B shows the same figure for a higher BPAP amplitude 100mV, which is the value used in \cite{shouval2002unified}.

Both curves are qualitatively similar to \cite{shouval2002unified} with two regions of LTD induction--one at negative values of $\Delta t$ and another in the region of $\Delta t > 100\mathrm{ms}$--and a single region of potentiation in between. However, for BPAP amplitude 100mV, the curve is smoother and weight differences larger, indicating that BPAP amplitude can be used for tuning STDP curve as necessary. In all following simulations, BPAP amplitude is fixed to 42mV to adhere to parameters published in \cite{yeung2004synaptic}.




\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/valid_stdp_bpap42.eps}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/valid_stdp_bpap100.eps}
\end{minipage}
\caption{\textbf{A.} STDP curve for BPAP amplitude at 42mV. The relative timing of pre- and postsynaptic spikes $\Delta t = t_{post} - t_{pre}$ is plotted on the x-axis, and the weight resulting from a 100-second 1Hz stimulation of pre-post spike-pairs with the respective $\Delta t$ is on the y-axis. The dashed line shows the value to which synaptic weights converge in the absence of postsynaptic spikes.
\textbf{B.} STDP curve for BPAP amplitude at 100mV. Note the change in scale.}
\label{fig:valid_stdp}
\end{figure}




\subsection{Metaplasticity}

The simplest effect of metaplasticity studied by \cite{yeung2004synaptic} was a slow scaling down of weights after a fast potentiation, or scaling up of weights after a fast depression. In both cases, the fast change is induced by regular synaptic plasticity, and the subsequent scaling my metaplasticity. In Figure~\ref{fig:valid_metaplasticity_evolution}, these dynamics are visible: after an initial quick potentiation, weights are slowly scaled down to a value where they remain stable.

The model produces two more effects of metaplasticity observed in \cite{yeung2004synaptic}. Firstly, it increases the variance in weights so that rather than weights being nearly for all synapses, the distribution is wider. Secondly, after the observed initial quick increase in weights, they are scaled down to a stable value.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/valid_metaplasticity_evolution.eps}
    \caption{Evolution of synaptic weights in a metaplasticity experiment. For weights of excitatory synapses, both the mean (blue line) and $\pm$1 standard deviation range (grey area) are shown. Input to all synapses was uncorrelated with a mean rate of 20Hz.}
    \label{fig:valid_metaplasticity_evolution}
\end{figure}




\subsection{Selectivity to temporal correlation} % aka temporal code

Temporal correlation between spike trains, i.e. increased probability of coincidence of spikes, indicates underlying structure in the input. Thus, if the neuron is to learn this structure, in the presence of two groups of inputs with only one group correlated, the synapses of correlated inputs should be selectively potentiated. A strong effect of this sort is observed in \cite{yeung2004synaptic}, with weights of the uncorrelated channel going to zero. However, it can be seen in Figure~\ref{fig:selectivity_correlation} that in the model used in this thesis, the selectivity to correlation is weak: the difference in weights of the two channels is existent but small, the group mean weights differing by approximately 20\%. An informal parameter sweep was conducted, but selectivity to correlation did not increase.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/valid_selectivity_correlation.eps}
    \caption{Evolution of synaptic weights over time in a selectivity experiment. Synapses 1-25 receive correlated input ($c=0.8$) at 30Hz, synapses 26-100 receive uncorrelated input at 30Hz. The values of synaptic weights were sampled every 100ms.}
    \label{fig:selectivity_correlation}
\end{figure}









\section{Learning patterns of input rate}

As shown in Section~\ref{subsec:iocurve}, the neuron's output rate is dependent on its input rate. In this section, I will study the neuron's ability to learn and respond to patterns encoded in the mean firing rates of inputs. First, it will be shown that the neuron distinguishes between learned inputs and noise. Secondly, the neuron will be shown to be linear upon presenting learned input patterns partially. Finally, the effect of pattern width on learning will be studied.

The approach taken will be black-box: parameters have fixed values and the only aspect varied is the input. The output will then be studied to make inferences about the neuron's information processing capabilities.

\subsection{Response to learned inputs}
\label{subsec:learnedinputs}
% Experiment 2 and Experiment 6: the neuron learns to fire more for a learned input than for a non-learned input.

% Experiment 2

Before investigating the neuron's ability to learn complex input patterns, it is necessary to ensure the neuron is capable of learning anything at all. In particular, it should be capable of something very simple: distinguishing between a learned pattern and random noise. To study this, a neuron was trained on a pattern (shown in Figure~\ref{fig:exp2_inputpatterns}A) consisting of high-rate (40Hz) input to 25 synapses, and low-rate (10Hz) background input to all other synapses, both the pattern and background uncorrelated.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/exp2_inputpatterns.eps}
    \caption{Two input rate patterns. A. Synapses 1-25 receive a higher rate. B. Synapses 76-100 receive a higher rate.}
    \label{fig:exp2_inputpatterns}
\end{figure}

After training, plasticity and metaplasticity were disabled by fixing the weights, and two tests were run. In Test 1, the neuron received exactly the same input as in training. As control, in Test 2, the high-rate inputs were shown to a different, non-overlapping set of synapses, with all other synapses receiving background input at 10Hz (shown in Figure~\ref{fig:exp2_inputpatterns}B). The total amount of spikes received by the neuron in both tests remained constant. The resulting output rate in a 5-second simulation was 64Hz for Test 1 and 44Hz for Test 2, suggesting that the neuron fired more when it was presented with the learned input (compared to non-learned input).

% Experiment 6

It is also important to understand the neuron's response when input varies between trained and untrained input. To this end, the input was alternated so pattern \ref{fig:exp2_inputpatterns}A was shown in time periods $t \in [1\mathrm{s}, 2\mathrm{s}) \cup [3\mathrm{s}, 4\mathrm{s}) \cup \ldots $. In between presentations of the trained pattern, all synapses received input at a rate of 17.5Hz ("flat" input), keeping the total input received by the neuron constant. The resulting instantaneous output rate, shown in Figure~\ref{fig:exp6voltageoscillation}, clearly oscillates between a high rate and a low rate. In addition, the mean firing rate in periods of learned input is significantly higher.

\begin{figure}[!htb]
    \includegraphics[width=\textwidth]{figures/exp6_voltageoscillation.eps}
    \caption{The evolution of firing rate over time. To find instantaneous firing rates, a sliding time window of length $L$ was chosen, and for each timestep $t$, instantaneous firing rate was calculated as the firing rate in time window $[t-\frac{L}{2}, t+\frac{L}{2})$.
    A. For $L=100\mathrm{ms}$, little to no oscillation is visible. B. For $L=1000\mathrm{ms}$, a clear oscillation is seen. The triangular shape of oscillation is caused by the method of calculating instantaneous firing rate. The top dashed line shows mean output rate over all time periods when the neuron was shown trained input; the bottom dashed line shows the mean over all time periods when the neuron was shown flat input.}
    \label{fig:exp6voltageoscillation}
\end{figure}

\begin{figure}[!htb]
\centering
\begin{minipage}[t]{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/exp1_filter.eps}
  \captionsetup{width=0.95\linewidth}
  \captionof{figure}{Response to varying intensity of learned input. Each datapoint shows output rate in a 5-second simulation. Intensity was varied by giving synapses 1-25 (those trained on a high rate) uncorrelated input $r$ varying from 0Hz to 70Hz, and giving all other synapses such a rate $r_b$ that the total input to the neuron remained constant at 1750Hz. The dotted line shows the value of $r=r_b=17.5\mathrm{Hz}$.}
  \label{fig:exp1_filter}
\end{minipage}%
\begin{minipage}[t]{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/exp5_patterncompletion.eps}
  \captionsetup{width=0.95\linewidth}
  \captionof{figure}{Response to varying proportions of trained pattern present. Each datapoint shows output rate in a 5-second simulation. On the x-axis, $p$ is defined as the proportion of the original 40 high-rate synapses receiving high-rate input in the test. Total input was normalised to 1750Hz by choosing an appropriate rate for all other synapses.}
  \label{fig:exp5_patterncompletion}
\end{minipage}
\end{figure}







\subsection{Response to partial presentation of learned inputs}
\label{subsec:partialpatterns}
% Experiment 1 and Experiment 5: the neuron responds linearly to varying input intensity (Exp 1) and proportion of pattern present (Exp 5).

% Experiment 1

An important aspect of the behaviour of a neuron is its ability to recognise partial or incomplete inputs. To study this, a neuron was trained on the pattern shown on Figure~\ref{fig:exp2_inputpatterns}A, and plasticity disabled. In the testing phase, intensity of the learned pattern was varied and output rate recorded. The results, given in Figure~\ref{fig:exp1_filter}, indicate that the neuron responds linearly to changing pattern intensity. What's more, the response is linear even when the pattern has negative intensity (the background rate is higher than pattern rate), i.e. the neuron linearly determines the absence of input. For control, a neuron trained on flat input of 17.5Hz was tested; the control neuron did not exhibit a strong systematic response.

%Basically, I vary the intensity of a learned input pattern and see what output the neuron produces. I think the result is interesting in that  shows what filter the neuron implements - a linear one. This can be compared with what is done in ANNs where they have rectified linear or sigmoidal filters. It seems to me that the analogy holds very well, and that this result is very useful in the context of building a network of these neurons (as well as general understanding of how the neuron behaves).

% Experiment 5

In addition to varying intensity of the learned pattern, one can show partial input by replacing some of the signal-carrying synapses with noise. For this, a neuron was trained on input consisting of a 40-synapse high-rate (40Hz) channel, with total input normalised to 1750Hz by choosing an appropriate input rate for the other synapses. As shown in Figure~\ref{fig:exp5_patterncompletion}, this neuron was tested by reducing the number of signal-carrying synapses. Again, the neuron exhibited linear behaviour: output rate increased linearly with the proportion of trained pattern present. For control, a neuron trained on flat input of 17.5Hz was tested; the control neuron did not exhibit a strong systematic response.

%I think the most interesting result here is that
%1. the neuron doesn't do pattern completion (which I anticipated since with learning disabled, the neuron fires more when there is more input) and
%2. the neuron responds to removing synapses pretty linearly, similar to its linearity when decreasing the input rate (experiment 1).






\subsection{Selectivity to patterns with different channel widths}
\label{subsec:patternwidths}

% Experiment 4: the neuron becomes more selective to patterns coded in a particular number of synapses (or a small range).

Running any simulation of a neuron requires specifying the input, with several parameters to vary: number of synapses, rate, correlation, number and size of distinct channels among others. Here, the effect of pattern width on output rate is studied. A neuron is trained by distributing a total of 1750Hz of input among two channels: 1000Hz evenly among synapses $1 \ldots w$ (high-rate channel), and 750Hz evenly among synapses $w \ldots 100$. For each value of $w$, a new neuron is trained, plasticity is disabled and the neuron tested on the training pattern. As control, for each value of $w$, a neuron trained on flat input of 17.5Hz ('untrained' neuron) is tested with the test pattern corresponding to $w$. The results are shown in Figure~\ref{fig:exp4synapsecount}.

Evidently, the neuron has a maximal response rate at $w\in[20,25]$. However, relative difference between the response of a trained neuron and that of an untrained neuron is a better measure for selectivity, as it shows the effect of learning on output rate. This difference is largest in the range $w \in [10,20]$, suggesting that a pattern of input should cover a small proportion of all available synapses to elicit maximal selectivity. It should be noted, though, that the effect of pattern width on output rate is low (less than 10Hz) compared to other parameters varied before.



\begin{figure}[!htb]
\centering
\begin{minipage}[t]{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/exp4_synapsecount.eps}
  \captionsetup{width=0.95\linewidth}
  \captionof{figure}{Learning patterns represented in varying numbers of synapses. Each datapoint shows mean output rate over 9 test runs lasting 5 seconds; the dashed lines show standard deviation of output rates in these 9 experiments.}
  \label{fig:exp4synapsecount}
\end{minipage}%
\begin{minipage}[t]{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/exp7_pr_destroying.eps}
  \captionsetup{width=0.95\linewidth}
  \captionof{figure}{The destructive effect of PR on effective input correlation. Each of the four coloured lines show effective correlation $c_{eff}$ for the respective values of probability of release $p$ and correlation $c$. $c_{eff}$ is calculated by generating 100 correlated (with coefficient $c$) spike trains, randomly removing each spike with probability $p$ and calculating mean of the correlations of all pairs of spike trains.}
  \label{fig:exp7_pr_destroying}
\end{minipage}
\end{figure}





\section{Probabilistic neurotransmitter release} % or [Investigating] the effect of probability of release

The effect of probabilistic neurotransmitter release (PR) was studied by varying the probability $p$ of neurotransmitter release upon a presynaptic spike. For all other simulations in this thesis, neurotransmitter is released from the presynaptic side upon every presynaptic spike, whereas for simulations in this section, the neurotransmitter is released with probability $p \in [0,1]$.

For uncorrelated Poissonian input of rate $r$, the only effect of PR is to decrease input rate, resulting in an effective input rate $r_{eff} = p \cdot r$. For correlated spiketrains, PR has an effect of destroying correlation in the input, as shown in Figure~\ref{fig:exp7_pr_destroying}. Thus, to study effects of PR not related to input rate correlated inputs must be studied.


% Experiment 8

An input-output curve similar to those in Section~\ref{subsec:iocurve} was produced for different values of $p$ with results shown in Figure~\ref{fig:exp8gridoutputs}. The only effect of PR for $p \geq 0.4$ is, predictably, to decrease the rate. However, for $p=0.2$, the shape of the curve is slightly sigmoidal, which suggests that PR causes higher input rates to have a higher (relative) impact on the output rate, and suppresses lower input rates.

\begin{figure}[!htb]
    \includegraphics[width=\textwidth]{figures/exp8_gridoutputs_epsp05.eps}
    \caption{Input-output curves for various values of $p$. Each data point shows output rate in the last 5 seconds of a 100-second simulation. In all simulations, input spike trains were correlated with $c=0.4$. To avoid errors in numerical integration caused by very high input rates, EPSP amplitude was set to 0.5 and spike-frequency adaptation was disabled in these simulations.}
    \label{fig:exp8gridoutputs}
\end{figure}


% Experiment 7

The existence of PR in biological neurons implies that there exists a value of $p \neq 1$, for which some characteristic measure of information transfer is at its optimal value. Two such candidate measures are easy to test: output rate and variance in output rate. Figure~\ref{fig:exp7grid} shows the dependence of these two measures on $p$. At each input rate tested both the output rate and variance (shown as standard deviation) increase as $p$ approaches 1, suggesting no optimal value of $p$ for these two measures in the interval $p \in [0,1)$. However, at $p=0.8$, mean output rate is roughly equal to or slightly larger than output rate at $p=1$. This indicates that, for the values of parameters used, a value of $p$ lower than 1 is useful because a lower $p$ causes lower energy expenditure on spiking and thus a more efficient information transfer.

\begin{figure}[!htb]
    \includegraphics[width=\textwidth]{figures/exp7_PRoutputvariance_grid.eps}
    \caption{The dependence of output rate on PR parameter $p$. For each combination of rate and $p$, 10 5-second experiments were run, resulting output rates measured and mean and standard deviation over the 10 results calculated. In all simulations, plasticity was disabled, with 5 synaptic weights fixed at 1.0 and all others at 0.05. Spike-frequency adaptation was also disabled.}
    \label{fig:exp7grid}
\end{figure}








\section{Principal component analysis task} % or PCA tasks
%Speak about what I tried, how it failed, why it failed and whether PCA is even feasible (and what should be done to make it so). Or maybe some of this goes in the Discussion section.
The neuron's ability to perform PCA, can be tested in various ways. The approach taken in this thesis is to define two channels of input, each of which can in principle be specified independently. Channel 1 consists of half of the 100 synapses; channel 2 contains the other half. Input to the two independent channels is generated by choosing input rates $(r_1, r_2)$ from a multivariate Gaussian distribution with means $(\mu_1, \mu_2)=(10,10)$ and covariance matrix
$$
C =
\begin{pmatrix}
  10 & 1 \\
  1 & 0.5
 \end{pmatrix}
$$
with all rates in Hz, and using intra-channel correlation $c=0.5$. In this setup, each input pattern can be represented in a two-dimensional space, with $(r_1, r_2)$ specifying the independent coordinates. Sampling the Gaussian distribution described above, an ellipse forms in this input space, centered at $(\mu_1, \mu_2)$. Training a neuron on these sampled inputs is a formulation of a PCA task: if the weight vector were to align with one of the axes of the ellipse, the neuron can be said to recognise one of the principal components, i.e. perform PCA. However, as shown in Figure~\ref{fig:exp10_pca}, the weights do not align as required for success in this setup. In informal parameter searches varying $\mu_1, \mu_2$ and $C$, no principal components were recognised either.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figures/exp10_pca.eps}
    \caption{Input space and result of PCA task. The 60 blue dots represent inputs shown to the neuron, each for 1s. The green line is the best-fit linear trend line for inputs. The red line is the weight vector: x- and y-components are calculated as mean of channel 1 and 2 weights, respectively. Mean output rate was measured as 29Hz. Spike-frequency adaptation was disabled and EPSP amplitude set to 0.5mV in this experiment.}
    \label{fig:exp10_pca}
\end{figure}










% % % %
% Chapter: Discussion
% % % %
\chapter{Discussion}
% TODO Mention this: http://www.cell.com/neuron/abstract/S0896-6273%2815%2900210-X

\section{Interpretation of results}
mina vs 2004/2002

\section{Limitations}
miks mul korrelatsioon ei tööta -- kitsam STDP tekitab correlation specificityt, mul on suht lai STDP

\section{Future work}







näitan, mis neuroni reaalne output on (mitte ainult weightid) -- see on väga oluline, et saaks networke ehitada


COMMENT on high firing rates: this is something I did contemplate myself. I think the main culprits are the exact values of two parameters: EPSP voltage and the stable point of weights. Drawing a plot that shows an input-output curve for a bunch of different EPSP values should take no more than 10 minutes (I have all the code), so I'll put this figure along with a description of other likely causes somewhere into Results > Validation. A weak justification is also that Shouval 2004 have high rates too (at least for an untrained neuron - 30Hz uncorrelated input results in 60Hz output).

Thinking of the result of experiment 1, there is actually also an implication for probabilistic release. Due to the linearity of the neuron, if there is a 50\% probability of release (effective input rate is 50\% of actual rate R), the output rate of the neuron will also be 50\% of the way between baseline and what one would expect for a rate R.

Using the graph \ref{fig:exp4synapsecount} from Experiment 4, I would argue that it does make a difference exactly how you create the pattern you're going to use with this model -- and coding a pattern in 10-20 synapses elicits selectivity well, whereas a 40-50 synapse pattern doesn't. This makes sense, since a few very strongly firing inputs seem to convey better information than a large amount of inputs that have only slightly higher rates than background.
The value of this knowledge: it helps inform further experiments by showing what kind of parameters to use.

In Selectivity to correlation subsection, selectivity was not achieved with any of the parameter sets tried in an informal sweep.

Compare prob. release results with what was found in (Prior work section article).






%Goals:
%1. implement the neuron model and validate its behaviour (against original papers and what is realistic)
%2. explore the behaviour of this model
%3. find what effects PR has on behaviour
%4. find out whether the neuron can perform a simple PCA task


\subsubsection{Implementation and validation}
1. The input-output curves, STDP curve and metaplasticity evolution (the graph showing that metaplasticity pulls weights down after an initial potentiation) look qualitatively similar to Shouval's, or at least they can be made similar upon changing the parameters a bit (for STDP curve). A significant difference in my model is that selectivity to correlation is much weaker, however I hypothesize that by playing with the STDP curve (which can be done by changing parameters affecting NMDA currents) will produce a stronger correlation selection effect, and this is a hypothesis that can be easily tested in further research. The output rates are high compared to biological neurons, but again this (I show) is affected a lot by the EPSP amplitude used.

\subsubsection{The neuron implements a linear filter}
The neuron clearly learns. Furthermore, the learning can be verified from the output rate of the neuron, which means meaningful information processing takes place. It responds near-linearly when shown partial input (in two different ways), so it can be said that the neuron implements a linear filter (context: ANNs' sigmoid or rectified linear filters). This is good to know for building networks of neurons.
I also find that there is some range of pattern width (number of synapses carrying signal) that is better than other widths - this directly informs further simulations for choosing patterns.

\subsubsection{PR increases efficiency of information transfer}
3. Probability of release 0.8 is, in some cases, equivalent to PR 1. This means it is indeed useful in that case, saving energy by eliminating unnecessary spikes (at least ones from a correlated source), which is in agreement with the conclusion of \cite{goldman2004enhancement} about PR increasing information transfer efficiency. It is possible that the results I have are not generalisable - that they are specific to the values of  {rate, weights} I used - but this is again something to look into in further research. The model could also be used to see if there is an information-theoretic metric whose maximum is at a medium PR value (e.g. 0.6), implying that PR is better than no PR.

\subsubsection{The neuron fails rate-based PCA at these parameter values}
4. The two PCA experiments I tried didn't work. However, a PCA task can be formulated in many ways, and my experiments with purely rate-based patterns definitely don't disprove the neuron's ability to do PCA. Rather, changing the STDP curve and increasing selectivity to correlation might have an effect and could be studied, and a more fundamental/theoretic approach to why and in what conditions the neuron could do PCA would be in order (similar to analysis of Oja rule -- but with simulations). In hindsight, trying to do PCA at first seems to have been a too large leap, too disconnected from what I already knew about the neuron (since PCA is a nontrivial algorithm for such a system to implement).


% % % %
% Conclusion
% % % %

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

%TODO add

The author of this thesis was responsible for implementing and validating the model, running simulations and analysing the resulting data. The research questions as well as components of the neuron model used were partly provided by the supervisors.

% % % %
% Bibliography
% % % %

\bibliographystyle{alpha}
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{Thesis}
Internet URLs were valid on May 14, 2015. % TODO remove if no URLs were cited
\newpage





% % % %
% Appendices
% % % %

\chapter*{Appendix A: MATLAB code}
\addcontentsline{toc}{chapter}{Appendix A: MATLAB code}
\label{appendix:code}

The MATLAB scripts for simulations and data analysis performed in this thesis are included with this thesis as an online supplementary material. The material will be withheld from publication until 26.06.2016.







% % % %
% Glossary
% % % %
\chapter*{Appendix B: Glossary}
\addcontentsline{toc}{chapter}{Appendix B: Glossary}
\label{appendix:glossary}

\begin{description}
  \item[ANN] artificial neural network
  \item[BPAP] back-propagating action potential
  \item[EPSP] excitatory postsynaptic potential
  \item[GABA] gamma-aminobutyric acid
  \item[IPSP] inhibitory postsynaptic potential
  \item[NMDA] N-Methyl-D-aspartic acid
  \item[NMDAR] NMDA receptor
  \item[PCA] principal component analysis
  \item[PR] probabilistic [neurotransmitter] release
  \item[STDP] spike-timing-dependent plasticity
\end{description}




% % % %
% Licence
% % % %

\chapter*{Licence}
\addcontentsline{toc}{chapter}{Licence}


%
% Non-exclusive licence to reproduce thesis and make thesis public
%
\section*{Non-exclusive licence to reproduce thesis and make thesis public}
I, Taivo Pungas (21.02.1994), 
\begin{enumerate}
	\item herewith grant the University of Tartu a free permit (non-exclusive licence) to:
	\begin{enumerate}[label*=\arabic*.]
		\renewcommand{\theenumi}{\arabic{enumi}}
		\item reproduce, for the purpose of preservation and making available to the public, including for addition to the DSpace digital archives until expiry of the term of validity of the copyright, and
		\item make available to the public via the web environment of the University of Tartu, including via the DSpace digital archives, as of 26.06.2016 until expiry of the term of validity of the copyright,
	\end{enumerate}
	``\thesistitle", supervised by Raul Vicente and Jaan Aru,
	
	\item I am aware of the fact that the author retains these rights.

	\item I certify that granting the non-exclusive licence does not infringe the intellectual property rights or rights arising from the Personal Data Protection Act. 
\end{enumerate}

Tartu, 14.05.2015

\thispagestyle{empty}
\newpage

\end{document}






















