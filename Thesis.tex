\documentclass[a4paper,12pt]{report}

%
% Bunch of useful packages
%
\usepackage[utf8]{inputenc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{pst-plot}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{floatflt}
\usepackage{wrapfig}
\usepackage{endnotes}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{endnotes}
\usepackage{pdflscape}
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{array}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{enumitem}

\usepackage[font={small, sl}]{caption}

%
% Listings
%
\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
\lstset{
	basicstyle=\scriptsize\ttfamily,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
	tabsize=2,
	extendedchars=true,
	breaklines=true,
	keywordstyle=\color{red},
	frame=b,         
	stringstyle=\color{white}\ttfamily,
	showspaces=false,
	showtabs=false,
	xleftmargin=17pt,
	framexleftmargin=17pt,
	framexrightmargin=5pt,
	framexbottommargin=4pt,
	showstringspaces=false
}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}

%
% Chapter titles
%
\usepackage{titlesec}
\titleformat{\chapter}[hang] 
{\normalfont\huge\bfseries}{\thechapter}{1em}{} 

%
% Theorems and Definitions
%
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]

%
% Margins of all kinds
%
\hypersetup{pdfborder={0 0 0 0}}
\setlength\parindent{0mm}
\setlength\parskip{3mm}

\newenvironment{pitemize}{
\vspace{-5mm}
\begin{itemize}
 	\setlength{\itemsep}{1pt}
	\setlength{\parskip}{0pt}
 	\setlength{\parsep}{0pt}
}{
	\end{itemize}
	\vspace{-8mm}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Width of A4 is 8.27in (210mm) We have an inside margin of 1.5 in and outside margin of 1in
%% This leaves 5.77in for text width
\oddsidemargin=0.5in
\evensidemargin=0.0in
\textwidth=5.77in
\headheight=0.0in
\topmargin=0.0in
\textheight=9.0in
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newenvironment{sitemize}
{\vspace{-2mm}\begin{list}{\textbullet}{%
    \setlength{\itemsep}{0pt}%
    \setlength{\parsep}{0pt}%
    \setlength{\topsep}{0pt}%
    \setlength{\parskip}{0pt} %
    \setlength{\labelwidth}{0pt}%
    \setlength{\labelsep}{0.05in} %
    \setlength{\leftmargin}{5pt} %
    \renewcommand{\labelitemi}{\textbullet}}}
  {\vspace{-2mm}\end{list}}

\newenvironment{mitemize}
{\begin{list}{\textbullet}{%
    \setlength{\itemsep}{0pt}%
    \setlength{\parsep}{0pt}%
    \setlength{\topsep}{2mm}%
    \setlength{\parskip}{0pt} %
    \setlength{\labelwidth}{0pt}%
    \setlength{\labelsep}{0.05in} %
    \setlength{\leftmargin}{8.5mm} %
    \renewcommand{\labelitemi}{\textbullet}}}
  {\end{list}}


\newcommand\epigraph[3]{
\vspace{1em}\hfill{}\begin{minipage}{#1}{\begin{spacing}{0.9}
\small\noindent\textit{#2}\end{spacing}
\vspace{1em}
\hfill{}{#3}}\vspace{2em}
\end{minipage}}


%
% Title
%
% TODO title would better be a bit shorter. Other possibilities:
% Biologically realistic single neuron model performs unsupervised learning

\begin{document}
\begin{center}
	{\Large
	University of Tartu\\
	Faculty of Mathematics and Computer Science\\
	Institute of Computer Science\\
	Computer Science\\}
	
	\vspace{2.5cm}
	
	{\LARGE Taivo Pungas}\\
	\vspace{0.5cm}
	\begin{spacing}{2}{\Huge\bf \ \ \ Unsupervised learning algorithms implemented by a biologically realistic neuron model}\end{spacing}
	\vspace{0.5cm}
	{\LARGE Bachelor's thesis (9 ECTS)}
\end{center}
\vspace{3cm}
\hspace{7.2cm}{\Large Supervisors: Dr. Raul Vicente\\}
\vspace{-0.5cm}

\hspace{10.4cm}{\Large Dr. Jaan Aru \\}

\ \\
\ \\
Author: .................................................. "......." May 2015\\
Supervisor: ............................................. "......." May 2015\\
\ \\
Approved for defense\\
Professor: ............................................... "......." May 2015\\
\ \\
\begin{center}
	{\Large Tartu 2015}
\end{center}
\thispagestyle{empty}
\pagebreak


%%%
% Abstract
%%%


{\textbf
{\Large Title of thesis in English}}

\textbf{Abstract:}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam a ultricies sem, vel gravida dolor. Vestibulum lacinia nulla nec massa mollis eleifend. Etiam scelerisque erat in ligula dapibus, et eleifend enim vulputate. Integer quis lobortis neque, eget efficitur felis. Vestibulum vel consequat quam. Praesent volutpat eget massa rutrum fringilla. Maecenas fringilla turpis ut dignissim luctus. Nunc lectus metus, dapibus eu enim ut, rhoncus blandit ligula. Praesent gravida ullamcorper est id pharetra. Mauris tempus ornare sollicitudin. Sed feugiat nec turpis mattis sollicitudin. Aliquam non orci sit amet lorem rutrum euismod et non orci. Aliquam egestas aliquam blandit. Cras quis convallis neque. Integer fringilla pharetra condimentum. Fusce erat eros, pulvinar sit amet orci in, consectetur accumsan eros.

\textbf{Keywords:} taivo, raul, jaan, model, neuro, stuff

\vspace{1.5cm}



{\textbf
{\Large Töö eestikeelne pealkiri}}

\textbf{Lühikokkuvõte:}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam a ultricies sem, vel gravida dolor. Vestibulum lacinia nulla nec massa mollis eleifend. Etiam scelerisque erat in ligula dapibus, et eleifend enim vulputate. Integer quis lobortis neque, eget efficitur felis. Vestibulum vel consequat quam. Praesent volutpat eget massa rutrum fringilla. Maecenas fringilla turpis ut dignissim luctus. Nunc lectus metus, dapibus eu enim ut, rhoncus blandit ligula. Praesent gravida ullamcorper est id pharetra. Mauris tempus ornare sollicitudin. Sed feugiat nec turpis mattis sollicitudin. Aliquam non orci sit amet lorem rutrum euismod et non orci. Aliquam egestas aliquam blandit. Cras quis convallis neque. Integer fringilla pharetra condimentum. Fusce erat eros, pulvinar sit amet orci in, consectetur accumsan eros.

\textbf{Võtmesõnad:} taivo, raul, jaan, mudel, neuro, värk


\
\thispagestyle{empty}
\pagebreak

\tableofcontents
\newpage


% % % %
% Chapter: Introduction
% % % %

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

% somehow maybe tie in x-risk and AGI?

Your brain is a complex organ capable of very sophisticated thought. Even though the chess-playing supercomputer Deep Blue won the reigning world champion Garry Kasparov, Kasparov was also able to do anything human beings do every day, whereas the computer could only play chess. The human brain is remarkable not for its ability to do one particular thing very well, be it playing chess, reading an article in Nature, preparing a six-course meal, or slam dunking in a basketball game. It is remarkable for being able to do all of those things, and much more.

However, gaining understanding of the brain's information processing mechanisms remains one of the major scientific challenges today. A key aspect of the brain is \emph{neural plasticity}, its ability to change itself over time as new knowledge and experience are accumulated and processed. To understand the dynamics of these changes, computational models are explored to understand how well they explain the brain's behaviour as observed in experiments in neuroscience, and on a more abstract level, how well the models learn to perform well in a variety of tasks.

Artificial systems with significant capacity for learning have recently been created using deep convolutional neural networks in tasks ranging from playing video games \cite{mnih2015human} to describing scenes in natural language \cite{karpathy2014deep}. The neuron models used in these systems are very simple, often composing of a scalar product of the inputs and weights, and a thresholding function. However, evidence from brains has shown that each neuron is a much more complex and powerful information processing unit. There is a clear gap between the computational properties of networks of neurons and the biophysical mechanisms of synaptic plasticity at the cellular level. The primary goal of this thesis is to help narrow this gap.

One way to bridge the gap is to show by numerical simulations that from basic principles of neuronal biophysics, more complex learning capabilities follow. Unsupervised learning is a mode of learning in which a system needs to learn the structure of input only based on the input itself, without being told explicitly what the input is. As an example of unsupervised learning in human subjects, one could show several pictures of zebras and tigers to a person previously unfamiliar with these animals, and the person would be able to distinguish that the animals shown fall into two major classes. While it has been shown that networks of neurons can perform this kind of tasks, it has been largely unstudied whether much smaller units of computation are capable of unsupervised learning.

This work explores the behaviour of one particular previously published (with a Nobel-winning physicist among the authors) biophysically realistic neuron model, which has been shown to produce a range of neural plasticity phenomena. The thesis attempts to demonstrate that starting from very basic biophysical principles, the neuron model exhibits behaviour equivalent to principal component analysis (PCA), a well known unsupervised learning procedure.

% % % %
% Chapter: Background
% % % %

\chapter{Background and related work}


\section{Overview of related neuroscience}

% Biology of the human brain
\subsection{Neurobiology of the brain}
The brain, a part of the nervous system, is the central information processing organ in animals. The computational properties of the brain arise from the heavily interconnected networks and sub-networks of specialised cells called \emph{neurons}. It has been approximated \cite{herculano2009human} that the human brain consists of $10^{11}$ neurons with $10^{15}$ inter-neuronal connections called \emph{syn4apses}. This work focuses on modelling one of many different types of neurons - pyramidal neurons.


\subsection{Neurons as computational units}
Every model of a brain that aspires for biophysical reality must include a way to model the behaviour of neurons and synapses. Each neuron can be seen as a small unit performing computation on some inputs and producing corresponding output. There are many computational models of neurons, with the level of detail ranging from complex multi-compartment models to very simple models such as those used in many artificial neural networks (ANNs). 

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/fig1.pdf}
    \caption{Schematic of a pyramidal neuron.}
    \label{fig:pyramidal}
\end{figure}

Information flow through a neuron follows a path from the inputs to the output, with integration of information in between, as shown in Figure~\ref{fig:pyramidal}. A rough mapping of the corresponding neuron anatomy is also shown.


\subsubsection{Input}

The region of connection between two neurons is called a \emph{synapse}, with information flowing from the axon of the \emph{presynaptic} neuron to a dendrite of the \emph{postsynaptic} neuron. Each time the presynaptic neuron produces significant output, neurotransmitters are released from the presynaptic neuron into the \emph{synaptic cleft}, a small gap between the presynaptic and postsynaptic neurons at the synapse.
Neurotransmitters then bind to receptors on the dendrite of the postsynaptic neuron, causing ion channels to open. This causes ion flow across the membrane, which causes a voltage change at the dendrite of the postsynaptic neuron.

Synapses can be divided into two classes: input arriving at \emph{excitatory} synapses tends to increase the output of the postsynaptic neuron, input to \emph{inhibitory} synapses tends to make it fire less.

\subsubsection{Computation}
A single neuron can be viewed as an integrator of information over time, over different inputs, or both. At each synapse, a neuron has some intrinsic biological components that determine how much this synapse should affect the output of the postsynaptic neuron. \emph{Synaptic weights} are key parameters used in modelling such biological components, and the modification of synaptic weights is the basis of learning in neurons.

After the contribution of input from each synapse is computed, this information must be integrated to produce the output of the neuron. An overview of some of the most common neural models of information integration will be given in Section~\ref{modelsofintegration}.

\subsubsection{Output}

Output of a neuron takes the form of \emph{action potentials} or \emph{spikes}, rapid increases in membrane voltage followed by a return to the equilibrium voltage. Information about action potentials is conducted from a neuron to the dendrites of other neurons using the axon. Axons are usually not part of single neuron models as the output of the modelled neuron is studied in and of itself, so needs not be relayed to other neurons.

\section{Prior work}
%TODO Suggestions from Elena: more detail (obv), examples of published models, what I do differently, why I chose this particular model. Add references.


There are many computational models of neurons, with the level of detail ranging from compartmental to very simple models of a neuron such as those used in ANNs. Using these models, it is possible to simulate neurons in a computer and investigate the models' behaviour in a very controlled and detailed way. As opposed to real experiments that are hindered by many difficulties, using a simulation approach allows detailed investigation and control of input and all parameters.

Two aspects of neurons can be modelled and used in different combinations: the mechanism for translating input signals into output (information integration) and the mechanism for updating synaptic weights (synaptic plasticity).


\subsection{Models of neural information integration}
\label{modelsofintegration}

Some of the best known models, from least to most detailed, are the McCulloch-Pitts \cite{mcculloch1943logical}, integrate-and-fire (IF), and Hodgkin-Huxley \cite{hodgkin1952quantitative} models. In the MP model which is widely utilised in ANNs, a weighted sum of inputs is calculated, to which a sigmoid function is applied to produce the output of the neuron. In the IF and HH models the neuron is modelled as an electric circuit, and then corresponding differential equations are used to calculate the output of the neuron.

%\subsection{Models of synaptic plasticity TODO}

%Different kinds of possible learning rules. Simple ones such as Hebb's rule, Oja's rule, BCM theory. What properties these rules have been shown to have (what behaviours they produce -- PCA etc). Finding learning rules by minimising Fisher information, other information-theoretic measures, or some other objective function \cite{echeveste2014generating}.

%The perceptron \cite{rosenblatt1958perceptron} that works as a simple linear classifier was one of the earliest proposed computational models of the neuron. Since then, numerous models have been proposed. An important characteristic of a neuron model is its mechanism of learning, However, few models explicitly model the biochemistry and physiology leading to synaptic plasticity.


\subsection{Single neuron models in unsupervised learning tasks}

Several single neuron models have been shown to be capable of unsupervised learning: simple artificial neurons can do PCA when trained according to the Oja rule, the tempotron model \cite{gutig2006tempotron} is able to learn spike timing-based patterns, the SKAN model \cite{afshar2014racing} is capable of unsupervised classification of patterns. However, none of the aforementioned models are based on biophysical principles, and no other biophysical models have been shown to do unsupervised learning on the single neuron level.

%Yeung 2004 showed selectivity. But seems that most work about unsupervised learning focuses on networks, not biophysical single neurons.




% % % %
% Chapter: Methods
% % % %
\chapter{Methods}

\section{Input generation}
Inputs to the model are simulated as $N$ spike trains, one for each synapse, with a given mean firing rate $r$ specified independently for each synapse. The spike trains are produced in a homogeneous Poissonian process, i.e. the occurrence of a spike is independent of the time since the last spike, and the average rate of spikes remains constant. Uncorrelated inputs are generated using TODO CITE (HomoPoisSpikeGen function)?
Correlation between inputs is achieved by specifying the correlation parameter $0 \leq c \leq 1$, and using code from \cite{macke2009} to generate correlated homogeneous Poissonian spike trains.

FIGURE: seletan, misasi on spike (et räägin presünaptilistest spike'idest, aga reaalselt ei simuleeri presyn neuroneid)
    näitan 50 sünapsi ja ütlen, et ok, tahan et esimesed oleks kõvasti korreleeritud, järgmised oleks korreleerimata, aga suurema rate'ga, viimased oleks korreleerimata ja madala rate'iga
    genereerin vastavad input spiked ja näitan samas rasterploti (kus vasakul on märgitud ka, mis rate'i igaüks pidi saama)
    seletan, mida rasterplot tähendab ja et iga presünaptiline spike mõjutab postsünaptilist mingil viisil
FIGURE: desired input rates for 100 synapses; actual input rates.

\section{Neuron model used in simulations}

The neuron model used in this work follows the model of a single neuron published in \cite{yeung2004synaptic}. An integrate-and-fire model of information integration is used with a calcium-dependent model of synaptic plasticity \cite{shouval2002unified}. Due to the way information is represented in computers, time is discretised into time-steps of length $dt$ regardless of the continuous nature of the biophysical processes. No dendritic distance is simulated, i.e. the possibility that some synapses are situated further from the soma is not taken into account. In this section, an overview of the most important aspects of the model will be given. Exact values of the parameters used are given in Appendix~\ref{appendix:parameters}.
% TODO Appendix letter may change.
% TODO do we use metaplasticity? prolly not because don't want to simulate that long.


\subsubsection{Integrate-and-fire model}

The voltage of the postsynaptic neuron can be increased by excitatory postsynaptic potentials (EPSPs) caused by presynaptic spikes at excitatory synapses, or decreased due to inhibitory postsynaptic potentials (IPSPs) caused by presynaptic spikes at inhibitory synapses. In addition, a leak current is simulated, which tends to return voltage to the resting potential $V_{rest}$. When postsynaptic voltage $V_{post}$ reaches the threshold $V_{thres}$, a postsynaptic spike is recorded, and $V_{post}$ is instantaneously set to the spiking voltage $V_{spike}$. In the next time-step, $V_{post}$ is returned to the reset potential $V_{reset}$, from where it returns to the baseline voltage.

Excitatory and inhibitory synapses are modelled by ion gates controlled by the respective neurotransmitters, glutamate and gamma-aminobutyric acid (GABA). Upon each spike at an excitatory synapse, the opening of glutamate-controlled ion channels is simulated, with the degree of open-ness (TODO) simulated as a sum of two decaying exponentials peaking at the time of spike. The resulting ion flow tends to drive $V_{post}$ towards the excitatory reversal potential $V_{EPSP}$. Similarly, upon each spike at an inhibitory synapse, the opening of GABA-controlled ion channels is simulated, and the resulting ion flow tends to drive $V_{post}$ towards the inhibitory reversal potential $V_{IPSP}$. Spikes at excitatory synapses have a larger relative influence on $V_{post}$) due to the higher simulated conductivity (maximum throughput of ions) of glutamate-controlled ion channels. The three currents - one from excitatory synapses, another from inhibitory synapses, and the leak current - together determine the sign and extent of change in postsynaptic voltage.

%FIGURE: spatial and temporal summation of EPSP-s, causing a spike (action potential) vs not causing a spike. Also on same timeline, an IPSP. Below, show excitatory and inhibitory spikes as raster plots. I think Julius had a good figure for this.



















% was: "Learning"
\subsubsection{Plasticity}

In addition to the non-changing components mentioned in the previous section, the extent to which presynaptic spikes influence postsynaptic voltage is modelled by the synaptic weights $\boldsymbol{W}=(W_i)$, with $i$ denoting the number of the synapse. The modification of these weights is the basis of learning in this model: increasing $W_i$ will increase the effect spikes at the $i$-th synapse have on postsynaptic voltage, and vice versa. In this model, only excitatory synapses are plastic, i.e. inhibitory synapses remain unchanged.

%TODO use Shouval Scholarpedia article for explaining basis of learning (http://www.scholarpedia.org/article/Models_of_synaptic_plasticity#Calcium_dependent_models_of_bidirectional_synaptic_plasticity)
The mechanism for modifying synaptic weights follows the Ca\textsuperscript{2+}-dependent model from \cite{shouval2002unified}, where the aim was "to construct a model based on a minimal number of assumptions". The model can be said to detect coincidences of presynaptic and postsynaptic spikes: the weight of synapse $i$ will be increased if the presynaptic spike occurs right before the postsynaptic spike (i.e. there is reason to believe the postsynaptic spike was \emph{caused} by the input), and decreased otherwise. The process of spike-timing-dependent plasticity (STDP) captures this idea that learning should depend on the exact timing on postsynaptic and presynaptic spikes, on the length of time between the two spikes in particular. It was shown in \cite{shouval2002unified} that the plasticity model used in this work implements STDP in a way similar to what has been observed in experiments in neuroscience.

% FIGURE: stdp curve for the model

The mechanism for detecting coincidences relies on N-methyl-D-aspartic acid (NMDA) receptors. NMDA receptors control gates that can allow calcium ions to flow into the neuron, which in turn causes an increase of the synaptic weight. The extent to which the calcium channels open depend on two factors: concentration of glutamate and dendritic voltage caused by a back-propagating action potential (BPAPs). The former relays information about presynaptic spikes: if a spike has occurred recently, the level of glutamate at the synapse is at a high level. The latter signifies a postsynaptic spike: when the postsynaptic neuron fires, an increase in voltage is propagated to the dendrites. The influx of calcium is dependent on the exact timing of spikes because both glutamate concentration and BPAP decay in time. The synaptic weights' update depends on the level of intracellular calcium as shown in Equation~\ref{eq:weightupdate}.

\begin{equation}
\frac{dW_i}{dt} = \eta ([Ca]_i) (\Omega([Ca]_i) - W_i)
\label{eq:weightupdate}
\end{equation}

This mechanism allows the synaptic weights to increase when there was a suitably timed pair of presynaptic and postsynaptic spikes. However, \cite{shouval2002unified} show that the same mechanism also accounts for decreasing and stable synaptic weights: at medium levels of Ca\textsuperscript{2+}, weights are decreased, and at low levels of Ca\textsuperscript{2+}, synaptic weights remain unchanged; this is coded in the function $\Omega$. The learning rate $\eta$ also depends on calcium: higher levels of Ca\textsuperscript{2+} elicit larger changes in synaptic weights.

The phenomenon of synaptic weights increasing due to learning is named long-term potentiation (LTP), and similar decreasing is named long-term depression (LTD). LTD and LTP are the methods the neuron uses to assign different synaptic weights based on the input to that synapse. The exact shape of $\Omega$ and $\Eta$ are given in \cite{shouval2002unified}.


%There are two possible sources of increase in the dendritic voltage of the postsynaptic neuron: excitatory postsynaptic potentials (EPSPs) caused by presynaptic spikes at excitatory synapses and back-propagating action potentials (BPAPs) caused by the postsynaptic neuron firing. Postsynaptic voltage can decrease due to inhibitory postsynaptic potentials (IPSPs) caused by presynaptic spikes at inhibitory synapses, or passive decay that tends to return voltage to the resting potential.





















\section{Implementation details}

The model was implemented in MATLAB; source code is available on Github\footnote{https://github.com/taivop/neuron}.

viita, et osa simulatsioonidest tegin HPC klastris

For updating values according to the differential equations, the Euler method was used with a $0.1ms$ time-step length (chosen as the highest value allowing numerical stability). Voltage of the postsynaptic neuron and other (TODO which?) parameters were initialised to previously found equilibrium values at the start of each simulation to avoid unstable behaviour at the start of each simulation. The release of neurotransmitters caused by each presynaptic spike is assumed to last 1 ms. Input was generated in 1-second blocks for ease of implementation. The complexity of simulation is linear with respect to the total simulation time. The computation time required for $x$ seconds of simulated neuron time varied between $2x$ and $10x$ seconds depending on the amount of input to the neuron (on the machine used for simulation).

TODO also mention mistake made in 2002 paper.




% % % %
% Chapter: Results
% % % %
\chapter{Results}
\section{Validation of the implementation}

\subsection{Input-output relationship}

\subsection{Spike timing-dependent plasticity}
FIGURE: STDP curve (normalised mean final weight vs post-pre timing)


\subsection{Metaplasticity}


\subsection{Selectivity to temporal correlation} % aka temporal code











\section{Learning patterns of input rate}

Creating a coherent story of experiments
I think the order of presentation of these results would best be:
A. Experiment 2 and Experiment 6: the neuron learns to fire more for a learned input than for a non-learned input.
B. Experiment 1 and Experiment 5: the neuron responds linearly to varying input intensity (Exp 1) and proportion of pattern present (Exp 5).
C. Experiment 4: the neuron becomes more selective to patterns coded in a particular number of synapses (or a small range).


\subsection{Partial patterns}




\section{Probability of release}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/exp7_PRoutputvariance_grid.eps}
    \caption{Caption lalala.}
    \label{fig:exp7grid}
\end{figure}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/exp8_gridoutputs_epsp05.eps}
    \caption{Caption lalala.}
    \label{fig:exp8gridoutputs}
\end{figure}

% % % %
% Chapter: Discussion
% % % %
\chapter{Discussion}
Mention this: http://www.cell.com/neuron/abstract/S0896-6273%2815%2900210-X









% % % %
% Appendices
% % % %
\chapter*{Appendix A: Description of model parameters}
\addcontentsline{toc}{chapter}{Appendix A: Description of model parameters}
\label{appendix:parameters}

possibly add a table of parameters with their values and descriptions here

\chapter*{Appendix B: MATLAB code}
\addcontentsline{toc}{chapter}{Appendix B: MATLAB code}
\label{appendix:code}

The MATLAB scripts for simulations and data analysis performed in this thesis are included with this thesis as an online supplementary material. The material will be withheld from publication until 26.06.2016.







% % % %
% Glossary
% % % %
\chapter*{Glossary}
\addcontentsline{toc}{chapter}{Glossary}
ANN - artificial neural network
EPSP
IPSP
BPAP
STDP
PCA
ICA


% % % %
% Bibliography
% % % %

\bibliographystyle{alpha}
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{Thesis}
Internet URLs were valid on May 14, 2015.
\newpage


% % % %
% License
% % % %

\chapter*{License}
\addcontentsline{toc}{chapter}{License}


%
% Non-exclusive license to reproduce thesis and make thesis public
%
\section*{Non-exclusive license to reproduce thesis and make thesis public}
I, Ilya Kuzovkin (09.07.1988), 
\begin{enumerate}
	\item herewith grant the University of Tartu a free permit (non-exclusive licence) to:
	\begin{enumerate}[label*=\arabic*.]
		\renewcommand{\theenumi}{\arabic{enumi}}
		\item reproduce, for the purpose of preservation and making available to the public, including for addition to the DSpace digital archives until expiry of the term of validity of the copyright, and
		\item make available to the public via the web environment of the University of Tartu, including via the DSpace digital archives until expiry of the term of validity of the copyright,
	\end{enumerate}
	``A New Approach to Training Brain-Computer Interface Systems", supervised by Konstantin Tretyakov,
	
	\item I am aware of the fact that the author retains these rights.

	\item I certify that granting the non-exclusive licence does not infringe the intellectual property rights or rights arising from the Personal Data Protection Act. 
\end{enumerate}

Tartu, 20.05.2013

\thispagestyle{empty}
\newpage

\end{document}






















